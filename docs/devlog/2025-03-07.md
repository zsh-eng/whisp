# 2025-03-07

- [x] Set up mono repo with shadcn/ui
- [x] Set up WXT within the mono repo
- [x] Construct basic UI for calling Whisper and getting response
  - [x] Set up background script
  - [x] Set up content script UI
  - [x] Set up keyboard shortcut for triggering content script on any page

## Drawing the waveform

The simplest way to draw the waveform is to use a `canvas` element.

### Issue 1: Speed

It works, but notice that the waveform moves very quickly.
That's because the audio data is being sampled at a high rate.

### Issue 2: Misalignment

To slow down the waveform we can bucket out the audio data.
You might think that we can just take the last `bucketSize * numberOfLines` samples,
calculate the average amplitude for each bucket, and then draw that.

However, this doesn't work if the bucket size is not aligned with the length of the audio
data that is coming in.

Let's say we have a bucket size of 4, an array of 15 `Float32Array`s coming in,
and that our total number of lines we want to render is 10.
Then, our "window" of audio data will have shifted, and the previously calculated amplitudes will change.

Each "line" is supposed to be a fixed height once it's rendered, but if we use the approach
above, then the height might deviate from frame to frame due to the misalignment.

To solve this, we have to manage the audio data coming in ourselves,
and only flush the data to calculate the amplitude if we have a full bucket.

```ts
// Process complete buckets
while (bufferWindowRef.current.length >= bucketSize) {
  // Take a complete bucket
  const bucket = bufferWindowRef.current.splice(0, bucketSize);

  // Calculate amplitude for this bucket
  const amplitudes = bucket.map(
    (buffer) =>
      buffer.reduce((sum, val) => sum + Math.abs(val), 0) / buffer.length
  );

  const bucketAmplitude = Math.max(...amplitudes);
  amplitudesRef.current.push(bucketAmplitude);

  // Limit the number of buckets we keep
  if (amplitudesRef.current.length > maxAmplitudes) {
    amplitudesRef.current = amplitudesRef.current.slice(-maxAmplitudes);
  }
}
```

### Issue 3: Frame rate

Previously, when the waveform was at our original speed, it looked very smooth
when moving.

Now, the waveform is jittery. Why?

The reason is that the waveform was never actually being animated in the first place.
We were just re-drawing the canvas at a high enough rate that it looked smooth.

When we increase our bucket size, sure we slow the waveform down, but we also
decrease the frame rate of the waveform, making the movement look jittery.

To solve this, we need to animate the waveform by using `requestAnimationFrame`.

> I was at the gym and I realised that essentially the waveform is
> a time-series bar chart.
> Instead of moving the waveform whenever new data comes in,
> we instead need to be consistently translating it at a fixed
> rate based on the time that has passed.

#### [`requestAnimationFrame`](https://developer.mozilla.org/en-US/docs/Web/API/Window/requestAnimationFrame)

I learnt that with `requestAnimationFrame`, the browser calls the callback function
before the next repaint, allowing us to draw the waveform and translate it at a fixed rate.

The callback takes in a high resolution timestamp of the previous frame's end.

#### Audio Analyser

We use `getFloatTimeDomainData` to get the amplitude of the audio data.

If we wanted frequency data instead, we would use `getFloatFrequencyData`.

However, the data returned by `getFloatTimeDomainData` is only a subset of the total length of audio.

Let's say if we call the callback every 10ms, the audio
fragment might be less than that - it's a "snapshot" of the most recent audio buffer at that moment.

#### Storing the time of the audio data

We need to store the `timecode` from the `ondataavailable` callback in the MediaRecorder.
To know the length of each element of the audio blob array,
we need to take the difference between the previous and current timecode.

My idea is for the MediaRecorder to run every 10ms, and store the start and end timecode
of each audio chunk that happens.

#### Animating the waveform

We assume that the data is coming in every 10ms, at the rate that we set the original
callback for the MediaRecorder.

The speed that we move the waveform is arbitrary; we can set it to 1px per ms or as we see fit.

I had a misunderstanding of the data - the Float32Array
doesn't represent the audio data, but rather a sample
of the amplitude of the audio data at a given time.

This means we can just take the Float32Array, compute
the amplitude and associate the timecode with that
amplitude.

Consequently, it simplifies out implementation of the
waveform, as we can just render the amplitude at the
timecode with the correct height.

## Research

- [ ] Understand MediaRecorder API, MediaStream, AudioContext, AnalyserNode, etc.
